{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#!pip install -Uqq fastbook\n",
    "import fastbook\n",
    "fastbook.setup_book()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastbook import *\n",
    "from IPython.display import display,HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Deep Dive: RNNs\n",
    "    What we call a language model is a model that has been trained to guess what the next word in a text is (having read the ones before). This kind of task is called self-supervised learning: we do not need to give labels to our model, just feed it lots and lots of texts. It has a process to automatically get labels from the data, and this task isn't trivial: to properly guess the next word in a sentence, the model will have to develop an understanding of the English (or other) language. \n",
    "       Self-supervised learning is not usually used for the model that is trained directly, but instead is used for pretraining a model used for transfer learning. \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jargon: Self-supervised learning: Training a model using labels that are embedded in the independent variable, rather than requiring external labels. For instance, training a model to predict the next word in a text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "It's not at all obvious how we're going to use what we've learned so far to build a language model. Sentences can be different lengths, and documents can be very long. So, how can we predict the next word of a sentence using a neural network? Let's find out!\n",
    "\n",
    "We've already seen how categorical variables can be used as independent variables for a neural network. The approach we took for a single categorical variable was to:\n",
    "\n",
    "1. Make a list of all possible levels of that categorical variable (we'll call this list the vocab).\n",
    "2. Replace each level with its index in the vocab.\n",
    "3. Create an embedding matrix for this containing a row for each level (i.e., for each item of the vocab).\n",
    "4. Use this embedding matrix as the first layer of a neural network. (A dedicated embedding matrix can take as inputs the raw vocab indexes created in step 2; this is equivalent to but faster and more efficient than a matrix that takes as input one-hot-encoded vectors representing the indexes.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do nearly the same thing with text! What is new is the idea of a sequence. First we concatenate all of the documents in our dataset into one big long string and split it into words, giving us a very long list of words (or \"tokens\"). Our independent variable will be the sequence of words starting with the first word in our very long list and ending with the second to last, and our dependent variable will be the sequence of words starting with the second word and ending with the last word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the steps necessary to create a language model has jargon associated with it from the world of natural language processing, and fastai and PyTorch classes available to help. The steps are:\n",
    "\n",
    "1. Tokenization:: Convert the text into a list of words (or characters, or substrings, depending on the granularity of your model)\n",
    "2. Numericalization:: Make a list of all of the unique words that appear (the vocab), and convert each word into a number, by looking up its index in the vocab\n",
    "3. Language model data loader creation:: fastai provides an LMDataLoader class which automatically handles creating a dependent variable that is offset from the independent variable by one token. It also handles some important details, such as how to shuffle the training data in such a way that the dependent and independent variables maintain their structure as required\n",
    "4. Language model creation:: We need a special kind of model that does something we haven't seen before: handles input lists which could be arbitrarily big or small. There are a number of ways to do this; in this chapter we will be using a recurrent neural network (RNN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "When we said \"convert the text into a list of words,\" we left out a lot of details. For instance, what do we do with punctuation? How do we deal with a word like \"don't\"? Is it one word, or two? What about long medical or chemical words? Should they be split into their separate pieces of meaning? How about hyphenated words? What about languages like German and Polish where we can create really long words from many, many pieces? What about languages like Japanese and Chinese that don't use bases at all, and don't really have a well-defined idea of word?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because there is no one correct answer to these questions, there is no one approach to tokenization. There are three main approaches:\n",
    "\n",
    "Word-based:: Split a sentence on spaces, as well as applying language-specific rules to try to separate parts of meaning even when there are no spaces (such as turning \"don't\" into \"do n't\"). Generally, punctuation marks are also split into separate tokens.\n",
    "Subword based:: Split words into smaller parts, based on the most commonly occurring substrings. For instance, \"occasion\" might be tokenized as \"o c ca sion.\"\n",
    "Character-based:: Split a sentence into its individual characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jargon: token: One element of a list created by the tokenization process. It could be a word, part of a word (a subword), or a single character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Tokenization with fastai\n",
    "Rather than providing its own tokenizers, fastai instead provides a consistent interface to a range of tokenizers in external libraries. Tokenization is an active field of research, and new and improved tokenizers are coming out all the time, so the defaults that fastai uses change too. However, the API and options shouldn't change too much, since fastai tries to maintain a consistent API even as the underlying technology changes.\n",
    "\n",
    "Let's try it out with the IMDb dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Naim Cavin\\anaconda3\\lib\\site-packages\\cupy\\_environment.py:205: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Naim Cavin\\anaconda3\\lib\\site-packages\\cupy\\_environment.py:205: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Naim Cavin\\anaconda3\\lib\\site-packages\\cupy\\_environment.py:205: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Naim Cavin\\anaconda3\\lib\\site-packages\\cupy\\_environment.py:205: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Naim Cavin\\anaconda3\\lib\\site-packages\\cupy\\_environment.py:205: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Naim Cavin\\anaconda3\\lib\\site-packages\\cupy\\_environment.py:205: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Naim Cavin\\anaconda3\\lib\\site-packages\\cupy\\_environment.py:205: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Naim Cavin\\anaconda3\\lib\\site-packages\\cupy\\_environment.py:205: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from fastai.text.all import *\n",
    "path = untar_data(URLs.IMDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = get_text_files(path, folders = ['train', 'test', 'unsup'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Being a long-time fan of Japanese film, I expected'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=files[7].open().read()\n",
    "a[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#171) ['Being','a','long','-','time','fan','of','Japanese','film',',','I','expected','more','than','this','.','I','ca',\"n't\",'really','be','bothered','to','write','to','much',',','as','this','movie'...]\n"
     ]
    }
   ],
   "source": [
    "spacy = WordTokenizer()\n",
    "toks = first(spacy([a]))\n",
    "print(coll_repr(toks, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Once again Mr. Costner has dragged out a movie for far longer than necessar'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = files[0].open().read(); txt[:75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#187) ['Once','again','Mr.','Costner','has','dragged','out','a','movie','for','far','longer','than','necessary','.','Aside','from','the','terrific','sea','rescue','sequences',',','of','which','there','are','very','few','I'...]\n"
     ]
    }
   ],
   "source": [
    "spacy = WordTokenizer()\n",
    "toks = first(spacy([txt]))\n",
    "print(coll_repr(toks, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Once again Mr. Costner has dragged out a movie for far longer than necessary. Aside from the terrific sea rescue sequences, of which there are very few I just did not care about any of the characters. Most of us have ghosts in the closet, and Costner's character are realized early on, and then forgotten until much later, by which time I did not care. The character we should really care about is a very cocky, overconfident Ashton Kutcher. The problem is he comes off as kid who thinks he's better than anyone else around him and shows no signs of a cluttered closet. His only obstacle appears to be winning over Costner. Finally when we are well past the half way point of this stinker, Costner tells us all about Kutcher's ghosts. We are told why Kutcher is driven to be the best with no prior inkling or foreshadowing. No magic here, it was all I could do to keep from turning it off an hour in.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, spaCy has mainly just separated out the words and punctuation. But it does something else here too: it has split \"it's\" into \"it\" and \"'s\". That makes intuitive sense; these are separate words, really. Tokenization is a surprisingly subtle task, when you think about all the little details that have to be handled. Fortunately, spaCy handles these pretty well for us—for instance, here we see that \".\" is separated when it terminates a sentence, but not in an acronym or number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#9) ['The','U.S.','dollar','$','1','is','$','1.00','.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first(spacy(['The U.S. dollar $1 is $1.00.']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fastai then adds some additional functionality to the tokenization process with the Tokenizer class:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#207) ['xxbos','xxmaj','once','again','xxmaj','mr','.','xxmaj','costner','has','dragged','out','a','movie','for','far','longer','than','necessary','.','xxmaj','aside','from','the','terrific','sea','rescue','sequences',',','of','which'...]\n"
     ]
    }
   ],
   "source": [
    "tkn = Tokenizer(spacy)\n",
    "#tkn = Tokenizer(WordTokenizer())\n",
    "print(coll_repr(tkn(txt), 31))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there are now some tokens that start with the characters \"xx\", which is not a common word prefix in English. These are special tokens.\n",
    "\n",
    "For example, the first item in the list, xxbos, is a special token that indicates the start of a new text (\"BOS\" is a standard NLP acronym that means \"beginning of stream\"). By recognizing this start token, the model will be able to learn it needs to \"forget\" what was said previously and focus on upcoming words.\n",
    "\n",
    "These special tokens don't come from spaCy directly. They are there because fastai adds them by default, by applying a number of rules when processing text. These rules are designed to make it easier for a model to recognize the important parts of a sentence. In a sense, we are translating the original English language sequence into a simplified tokenized language—a language that is designed to be easy for a model to learn.\n",
    "\n",
    "For instance, the rules will replace a sequence of four exclamation points with a special repeated character token, followed by the number four, and then a single exclamation point. In this way, the model's embedding matrix can encode information about general concepts such as repeated punctuation rather than requiring a separate token for every number of repetitions of every punctuation mark. Similarly, a capitalized word will be replaced with a special capitalization token, followed by the lowercase version of the word. This way, the embedding matrix only needs the lowercase versions of the words, saving compute and memory resources, but can still learn the concept of capitalization.\n",
    "\n",
    "Here are some of the main special tokens you'll see:\n",
    "\n",
    "xxbos:: Indicates the beginning of a text (here, a review)\n",
    "xxmaj:: Indicates the next word begins with a capital (since we lowercased everything)\n",
    "xxunk:: Indicates the word is unknown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the rules that were used, you can check the default rules:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<function fastai.text.core.fix_html(x)>,\n",
       " <function fastai.text.core.replace_rep(t)>,\n",
       " <function fastai.text.core.replace_wrep(t)>,\n",
       " <function fastai.text.core.spec_add_spaces(t)>,\n",
       " <function fastai.text.core.rm_useless_spaces(t)>,\n",
       " <function fastai.text.core.replace_all_caps(t)>,\n",
       " <function fastai.text.core.replace_maj(t)>,\n",
       " <function fastai.text.core.lowercase(t, add_bos=True, add_eos=False)>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "defaults.text_proc_rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a brief summary of what each does:\n",
    "\n",
    "fix_html:: Replaces special HTML characters with a readable version (IMDb reviews have quite a few of these)\n",
    "\n",
    "replace_rep:: Replaces any character repeated three times or more with a special token for repetition (xxrep), the number of \n",
    "times it's repeated, then the character\n",
    "\n",
    "replace_wrep:: Replaces any word repeated three times or more with a special token for word repetition (xxwrep), the number of times it's repeated, then the word\n",
    "\n",
    "spec_add_spaces:: Adds spaces around / and #\n",
    "\n",
    "rm_useless_spaces:: Removes all repetitions of the space character\n",
    "\n",
    "replace_all_caps:: Lowercases a word written in all caps and adds a special token for all caps (xxup) in front of it\n",
    "\n",
    "replace_maj:: Lowercases a capitalized word and adds a special token for capitalized (xxmaj) in front of it\n",
    "\n",
    "lowercase:: Lowercases all text and adds a special token at the beginning (xxbos) and/or the end (xxeos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"(#11) ['xxbos','©','xxmaj','fast.ai','xxrep','3','w','.fast.ai','/','xxup','index']\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coll_repr(tkn('&copy;   Fast.ai www.fast.ai/INDEX'), 31)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at an example. For our corpus, we'll use the first 2,000 movie reviews:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "txts = L(o.open(encoding=\"utf8\").read() for o in files[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once our texts have been split into tokens, we need to convert them to numbers. We'll look at that next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numericalization with fastai\n",
    "Numericalization is the process of mapping tokens to integers. The steps are basically identical to those necessary to create a Category variable, such as the dependent variable of digits in MNIST:\n",
    "\n",
    "1. Make a list of all possible levels of that categorical variable (the vocab).\n",
    "2. Replace each level with its index in the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#207) ['xxbos','xxmaj','once','again','xxmaj','mr','.','xxmaj','costner','has','dragged','out','a','movie','for','far','longer','than','necessary','.','xxmaj','aside','from','the','terrific','sea','rescue','sequences',',','of','which'...]\n"
     ]
    }
   ],
   "source": [
    "toks = tkn(txt)\n",
    "print(coll_repr(tkn(txt), 31))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to call setup on Numericalize; this is how we create the vocab. That means we'll need our tokenized corpus first. Since tokenization takes a while, it's done in parallel by fastai; but for this manual walkthrough, we'll use a small subset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#207) ['xxbos','xxmaj','once','again','xxmaj','mr','.','xxmaj','costner','has'...]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks200 = txts[:200].map(tkn)\n",
    "toks200[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pass this to setup to create our vocab:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"(#1968) ['xxunk','xxpad','xxbos','xxeos','xxfld','xxrep','xxwrep','xxup','xxmaj','the','.',',','a','and','of','to','is','it','i','in'...]\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = Numericalize()\n",
    "num.setup(toks200)\n",
    "coll_repr(num.vocab,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our special rules tokens appear first, and then every word appears once, in frequency order. The defaults to Numericalize are min_freq=3,max_vocab=60000. max_vocab=60000 results in fastai replacing all words other than the most common 60,000 with a special unknown word token, xxunk. This is useful to avoid having an overly large embedding matrix, since that can slow down training and use up too much memory, and can also mean that there isn't enough data to train useful representations for rare words. However, this last issue is better handled by setting min_freq; the default min_freq=3 means that any word appearing less than three times is replaced with xxunk.\n",
    "\n",
    "fastai can also numericalize your dataset using a vocab that you provide, by passing a list of words as the vocab parameter.\n",
    "\n",
    "Once we've created our Numericalize object, we can use it as if it were a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorText([   2,    8,  349,  183,    8, 1176,   10,    8, 1177,   60, 1455,   62,   12,   25,   28,  189,  957,   93,  958,   10])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums = num(toks)[:20]; nums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, our tokens have been converted to a tensor of integers that our model can receive. We can check that they map back to the original text:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxbos xxmaj once again xxmaj mr . xxmaj costner has dragged out a movie for far longer than necessary .'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(num.vocab[o] for o in nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have numbers, we need to put them in batches for our model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have 90 tokens, separated by spaces. Let's say we want a batch size of 6. We need to break this text into 6 contiguous parts of length 15:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>xxbos</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>in</td>\n",
       "      <td>this</td>\n",
       "      <td>chapter</td>\n",
       "      <td>,</td>\n",
       "      <td>we</td>\n",
       "      <td>will</td>\n",
       "      <td>go</td>\n",
       "      <td>back</td>\n",
       "      <td>over</td>\n",
       "      <td>the</td>\n",
       "      <td>example</td>\n",
       "      <td>of</td>\n",
       "      <td>classifying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>movie</td>\n",
       "      <td>reviews</td>\n",
       "      <td>we</td>\n",
       "      <td>studied</td>\n",
       "      <td>in</td>\n",
       "      <td>chapter</td>\n",
       "      <td>1</td>\n",
       "      <td>and</td>\n",
       "      <td>dig</td>\n",
       "      <td>deeper</td>\n",
       "      <td>under</td>\n",
       "      <td>the</td>\n",
       "      <td>surface</td>\n",
       "      <td>.</td>\n",
       "      <td>xxmaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>first</td>\n",
       "      <td>we</td>\n",
       "      <td>will</td>\n",
       "      <td>look</td>\n",
       "      <td>at</td>\n",
       "      <td>the</td>\n",
       "      <td>processing</td>\n",
       "      <td>steps</td>\n",
       "      <td>necessary</td>\n",
       "      <td>to</td>\n",
       "      <td>convert</td>\n",
       "      <td>text</td>\n",
       "      <td>into</td>\n",
       "      <td>numbers</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>how</td>\n",
       "      <td>to</td>\n",
       "      <td>customize</td>\n",
       "      <td>it</td>\n",
       "      <td>.</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>by</td>\n",
       "      <td>doing</td>\n",
       "      <td>this</td>\n",
       "      <td>,</td>\n",
       "      <td>we</td>\n",
       "      <td>'ll</td>\n",
       "      <td>have</td>\n",
       "      <td>another</td>\n",
       "      <td>example</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>of</td>\n",
       "      <td>the</td>\n",
       "      <td>preprocessor</td>\n",
       "      <td>used</td>\n",
       "      <td>in</td>\n",
       "      <td>the</td>\n",
       "      <td>data</td>\n",
       "      <td>block</td>\n",
       "      <td>xxup</td>\n",
       "      <td>api</td>\n",
       "      <td>.</td>\n",
       "      <td>\\n</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>then</td>\n",
       "      <td>we</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>will</td>\n",
       "      <td>study</td>\n",
       "      <td>how</td>\n",
       "      <td>we</td>\n",
       "      <td>build</td>\n",
       "      <td>a</td>\n",
       "      <td>language</td>\n",
       "      <td>model</td>\n",
       "      <td>and</td>\n",
       "      <td>train</td>\n",
       "      <td>it</td>\n",
       "      <td>for</td>\n",
       "      <td>a</td>\n",
       "      <td>while</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stream = \"In this chapter, we will go back over the example of classifying movie reviews we studied in chapter 1 and dig deeper under the surface. First we will look at the processing steps necessary to convert text into numbers and how to customize it. By doing this, we'll have another example of the PreProcessor used in the data block API.\\nThen we will study how we build a language model and train it for a while.\"\n",
    "tokens = tkn(stream)\n",
    "bs,seq_len = 6,15\n",
    "d_tokens = np.array([tokens[i*seq_len:(i+1)*seq_len] for i in range(bs)])\n",
    "df = pd.DataFrame(d_tokens)\n",
    "display(HTML(df.to_html(index=False,header=None)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a perfect world, we could then give this one batch to our model. But that approach doesn't scale, because outside of this toy example it's unlikely that a single batch containing all the texts would fit in our GPU memory (here we have 90 tokens, but all the IMDb reviews together give several million).\n",
    "\n",
    "So, we need to divide this array more finely into subarrays of a fixed sequence length. It is important to maintain order within and across these subarrays, because we will use a model that maintains a state so that it remembers what it read previously when predicting what comes next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going back to our previous example with 6 batches of length 15, if we chose a sequence length of 5, that would mean we first feed the following array:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>xxbos</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>in</td>\n",
       "      <td>this</td>\n",
       "      <td>chapter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>movie</td>\n",
       "      <td>reviews</td>\n",
       "      <td>we</td>\n",
       "      <td>studied</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>first</td>\n",
       "      <td>we</td>\n",
       "      <td>will</td>\n",
       "      <td>look</td>\n",
       "      <td>at</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>how</td>\n",
       "      <td>to</td>\n",
       "      <td>customize</td>\n",
       "      <td>it</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>of</td>\n",
       "      <td>the</td>\n",
       "      <td>preprocessor</td>\n",
       "      <td>used</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>will</td>\n",
       "      <td>study</td>\n",
       "      <td>how</td>\n",
       "      <td>we</td>\n",
       "      <td>build</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bs,seq_len = 6,5\n",
    "d_tokens = np.array([tokens[i*15:i*15+seq_len] for i in range(bs)])\n",
    "df = pd.DataFrame(d_tokens)\n",
    "display(HTML(df.to_html(index=False,header=None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>,</td>\n",
       "      <td>we</td>\n",
       "      <td>will</td>\n",
       "      <td>go</td>\n",
       "      <td>back</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>chapter</td>\n",
       "      <td>1</td>\n",
       "      <td>and</td>\n",
       "      <td>dig</td>\n",
       "      <td>deeper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>the</td>\n",
       "      <td>processing</td>\n",
       "      <td>steps</td>\n",
       "      <td>necessary</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxmaj</td>\n",
       "      <td>by</td>\n",
       "      <td>doing</td>\n",
       "      <td>this</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>the</td>\n",
       "      <td>data</td>\n",
       "      <td>block</td>\n",
       "      <td>xxup</td>\n",
       "      <td>api</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>a</td>\n",
       "      <td>language</td>\n",
       "      <td>model</td>\n",
       "      <td>and</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bs,seq_len = 6,5\n",
    "d_tokens = np.array([tokens[i*15+seq_len:i*15+2*seq_len] for i in range(bs)])\n",
    "df = pd.DataFrame(d_tokens)\n",
    "display(HTML(df.to_html(index=False,header=None)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going back to our movie reviews dataset, the first step is to transform the individual texts into a stream by concatenating them together. As with images, it's best to randomize the order of the inputs, so at the beginning of each epoch we will shuffle the entries to make a new stream (we shuffle the order of the documents, not the order of the words inside them, or the texts would not make sense anymore!).\n",
    "\n",
    "We then cut this stream into a certain number of batches (which is our batch size). For instance, if the stream has 50,000 tokens and we set a batch size of 10, this will give us 10 mini-streams of 5,000 tokens. What is important is that we preserve the order of the tokens (so from 1 to 5,000 for the first mini-stream, then from 5,001 to 10,000...), because we want the model to read continuous rows of text (as in the preceding example). An xxbos token is added at the start of each during preprocessing, so that the model knows when it reads the stream when a new entry is beginning.\n",
    "\n",
    "So to recap, at every epoch we shuffle our collection of documents and concatenate them into a stream of tokens. We then cut that stream into a batch of fixed-size consecutive mini-streams. Our model will then read the mini-streams in order, and thanks to an inner state, it will produce the same activation whatever sequence length we picked.\n",
    "\n",
    "This is all done behind the scenes by the fastai library when we create an LMDataLoader. We do this by first applying our Numericalize object to the tokenized texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums200 = toks200.map(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = LMDataLoader(nums200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm that this gives the expected results, by grabbing the first batch:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 72]), torch.Size([64, 72]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y = first(dl)\n",
    "x.shape,y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then looking at the first row of the independent variable, which should be the start of the first text:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxbos xxmaj once again xxmaj mr . xxmaj costner has dragged out a movie for far longer than necessary .'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(num.vocab[o] for o in x[0][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dependent variable is the same thing offset by one token:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxmaj once again xxmaj mr . xxmaj costner has dragged out a movie for far longer than necessary . xxmaj'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(num.vocab[o] for o in y[0][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes all the preprocessing steps we need to apply to our data. We are now ready to train our text classifier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Text Classifier\n",
    "There are two steps to training a state-of-the-art text classifier using transfer learning: first we need to fine-tune our language model pretrained on Wikipedia to the corpus of IMDb reviews, and then we can use that model to train a classifier.\n",
    "\n",
    "As usual, let's start with assembling our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model Using DataBlock\n",
    "fastai handles tokenization and numericalization automatically when TextBlock is passed to DataBlock. All of the arguments that can be passed to Tokenize and Numericalize can also be passed to TextBlock. In the next chapter we'll discuss the easiest ways to run each of these steps separately, to ease debugging—but you can always just debug by running them manually on a subset of your data as shown in the previous sections. And don't forget about DataBlock's handy summary method, which is very useful for debugging data issues.\n",
    "\n",
    "Here's how we use TextBlock to create a language model, using fastai's defaults:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the 'c:\\users\\naim cavin\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install fastai -q --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: File \"setup.py\" or \"setup.cfg\" not found. Directory cannot be installed in editable mode: C:\\Users\\Naim Cavin\\Desktop\\python exercises\n",
      "WARNING: You are using pip version 21.1.1; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the 'c:\\users\\naim cavin\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install -e \".[dev]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_imdb = partial(get_text_files, folders=['train', 'test', 'unsup'])\n",
    "\n",
    "dls_lm = DataBlock(\n",
    "    blocks=TextBlock.from_folder(path, is_lm=True),\n",
    "    get_items=get_imdb, splitter=RandomSplitter(0.1)\n",
    ").dataloaders(path, path=path, bs=10, seq_len=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing that's different to previous types we've used in DataBlock is that we're not just using the class directly (i.e., TextBlock(...), but instead are calling a class method. A class method is a Python method that, as the name suggests, belongs to a class rather than an object. (Be sure to search online for more information about class methods if you're not familiar with them, since they're commonly used in many Python libraries and applications; we've used them a few times previously in the book, but haven't called attention to them.) The reason that TextBlock is special is that setting up the numericalizer's vocab can take a long time (we have to read and tokenize every document to get the vocab). To be as efficient as possible it performs a few optimizations:\n",
    "\n",
    "- It saves the tokenized documents in a temporary folder, so it doesn't have to tokenize them more than once\n",
    "- It runs multiple tokenization processes in parallel, to take advantage of your computer's CPUs\n",
    "\n",
    "\n",
    "We need to tell TextBlock how to access the texts, so that it can do this initial preprocessing—that's what from_folder does.\n",
    "\n",
    "show_batch then works in the usual way:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning the Language Model\n",
    "To convert the integer word indices into activations that we can use for our neural network, we will use embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Then we'll feed those embeddings into a recurrent neural network (RNN), using an architecture called AWD-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = language_model_learner(\n",
    "    dls_lm, AWD_LSTM, drop_mult=0.3, \n",
    "    metrics=[accuracy, Perplexity()]).to_fp16()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function used by default is cross-entropy loss, since we essentially have a classification problem (the different categories being the words in our vocab). The perplexity metric used here is often used in NLP for language models: it is the exponential of the loss (i.e., torch.exp(cross_entropy)). We also include the accuracy metric, to see how many times our model is right when trying to predict the next word, since cross-entropy (as we've seen) is both hard to interpret, and tells us more about the model's confidence than its accuracy.\n",
    "\n",
    "Let's go back to the process diagram from the beginning of this chapter. The first arrow has been completed for us and made available as a pretrained model in fastai, and we've just built the DataLoaders and Learner for the second stage. Now we're ready to fine-tune our language model!"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcsAAABuCAYAAABIpDiuAAAgAElEQVR4Ae2dB1hUV97/o/tutmb3/e9m67u72b7ZbMpmk2gSNWrsvcfewYa9YO9YY8XeG9gFC70jCkjvRUVFUQFBqUqd7//5HuayAw7TGIYZuDzPfe7MLefee+Ywn/n+2nkD8p/cA3IPyD0g94DcA3IPaOyBNzTulXfKPSD3gNwDcg/IPSD3AGRYyoNA7gG5B+QekHtA7gEtPSDDUksHybvlHpB7QO4BuQfkHpBhaWZjQKFQQFrM7Nbk22kCPaBQVEJRWS7GWBN4HPkRzKkHFAqgsqxq4esm9ifD0sw+UIKyuLgYL1++RGVlpZndnXw7lt0DClSWv0Tp81SUv3wmA9OyP0zzu3uCsiAVyAkFygqAJgZMGZZmNuQIy7y8PGRnZ6OoqEgGppl9PpZ9OwpUlhWi+HEQih76yMC07A/T/O6+sgR4dhNI2Qw8dW9ywJRhaWZDToLl48ePkZmZKYBZUVFhZncp345l9oASlo/8kJ/siOKHfkpgyhYMy/w8zeyuCcvsQCBmPhC3BHjq0aSAKcPSzMabKiwzMjKQlZUlA9PMPiPLvR0lLB/6IS/pBPKTT6PokT/Ki7OhqJR/kFnu52omdy7BMnoOEG4NxC1tUsCUYWkm40y6DQmWT548AWEpA1PqGXld/x6oDctTyEs5jeJHASgvzpKBWf8Obt4tqMIybIISmFSYnlUK08J7R4almX2A6mApA9PMPiSLvZ3XYZmffAr5BGZGoBKY5Rb7dPKNN3IPvAbLiUpgLlYCs7CRb7B+l5dhWb/+M/rZdcFSBqbRu7oZNlgHLJOVCjPjOsqLM0VqSTPsHPmR69sDamFJYFoBsYuATG+grNBio2RlWNZ3gBj5fE2wlIFp5M5uds3VDcv/KkwZmM1uWBjrgeuEpQTMhUCmj8UG/ciwNNZAMVI72mApA9NIHd0sm9ECS2GSPYPijCBZYTbL8VHPh9YISwJzIhBrC2T6AWX5FqcwZVjWc3wY+3RdYCkD09i93lza0wGWMjCby2Aw/nPqAsuwiUDMAiDLXwlMy0lbkmFp/CFTrxZ1haUqMOVKP/Xq8mZ0so6wlID5mAqTUbJy0E8zGiSGP6ousKS6ZKQsgZkdAJTmAQrLAKYMS8OHRoOcqQ8sJWCyPJ4MzAb5OJpYo3rAUgZmE/vsTfA4usJSFZhZgRYDTBmWJhhD+lxCX1jKwNSnd5v7sXrCUgKmiJKV8zCb++jR+vz6wNICgSnDUusIMO0BhsCSwGRpPFlhmvazsryrGQBLAUzmYTJKVgam5X3mJrxjfWFZA5jmb5I1O1gSFlxoVpReN6c1n/vFixdgbVhCUJ9FBqa2LwZF1fRUlWVQNMelohQVJS9EEfWqcnenIFJGCERtS3Xhgmxwmi/5T00PKCqAihKg4lXzXMryqlJDomdX+SUJQ10W4cO0BbIITPONkjUrWBKKr169ErNucOaN5rgQlISePpCUjn306BGePn0qFCb7Uv6r2QOVFSUozU3Cq6zI5rlkRuDlkxAU3HFS1obVAZKqEBXADJCLr9ccVlXvFOVA8UPg8TXg4YXmuaSfBVK2AhGT9YOlpDBjGSUbYLZpJWYHSwKSqopf/M19kSCoz1oGprpvMm5ToKI0HwVpV5CXdAp5SSeb/aJVTaqCUnot1ZKV58OsOdCoKDmPY9QsIGx8FSyomJrlonx+XVSl6jFSlGwW8zDNr9KP2cJSH0DIx75urqXC5ATSssKUvtP+C8v8JD0VlQQKeV1lrhXA9Jfnw5SGFtfVsNTTBKkKC/l11Y8LUbjAtwqYqn3cyK9lWOrpF7QkMMvAVP3vkmFpkJKs6wdCyhkUc3ovWWFWDTIZlrr5J3X9QRCrUhpP9d+4EV/LsGzCsCTYJR+mnIcpw9KosCREBTA5gTSDfpq5j1yGpXFhyeLrcZytxMtspveSYdnEYUlgylGy/Dkqw9LosCQwU88qFWYzB6YMSyPDUll8PY7zYSqB2cg/yMwOlkVFRXj27BmysrLElzwnQTYkjcKSzKWmuFcZmMwxLEbx45sovO+BwvuuKEy7goLbF8R8jvnJDshPOqk9haIus2Rz3i4DE6gshSI/CYrb9lAkrQMSVlVNSxU1A4iY9N9AH13NkPJxSvhaA3FLlfNhNm5aiclhSXNNRUUFCMWcnBwR8Xr79m0kJCQgNjYW4eHhYomMjBTvk5OTce/ePZFKwS98QpQAJWAY+WkK0DSVa7DvmnrhAkVlBcpe5qMw6w6epV7Ho7DzuOuzC8ku6xB/aRGiHaYg9owNEi7OQ6rLKjwIsMfTiFPITb6CgjR3FKZdRUHqOQh4NmcA6vvs1cB81qRNspXlpSh5noX827HIDvHEw2vHcffUFqQeWImkHbOQtHkMUrdPwN190/DQcSGyXDcgL2gHSmJ2oTJhExT0xUVOaQAVpmNOo0VCWBWYBY02W4lJYElAlpWVITc3F3fu3EFERAQCAgLg4eEBFxcXXL58GU5OTrh06RIuXrwoFr52dnbGlStX4OrqCi8vL1y/fh2EKOFKtUl4SuBsKkBr6OdoisCsrCjDyxdPkJXki9ue2xB53ApB27rBd+1n8Fz6T7gv+ANc5/4GLrN/gWuzfi7WrnN/DbcFf4DXkn/Ad81/ELSlIyIOj0DK1WV4fOsoXqReRcHdK8LM2CDmS31hZAnHN8mgHwUqXhWj8EEKHntfQMq+ZYhcMgzBk9ojcMS/4Tfgr/Dp+Tt4d/s1vLr8Al5d3oZ3t1/Cp+dv4dfvjwgc+h5ujv8UEXM7IXnzSGScW4LC4K0oj90IRcx8pepsyqAz0rNFWAPxyxpVYTYoLBlUQgX54MEDoRZ9fHwE/C5cuIAzZ87g9OnTcHR01GnhsefOnRNQJTz9/f0RFRWF+/fvy9DU0+/aNICpQHlJIV6kR+Ou316EHxkH/3VfCABesfkJnCe9CSerlrg0sQUuTXxDw9JCHOc8+U1cnf5TuNu+A/91rRF+cChuu61BdswZ5N+5olSbcsqJ1h8ONYBpuZV+WKWo9MUzZN/yFqoxfF4/BA77AN7dfgX3dj+E6+f/A5dWLeHy2Rual1Yt4Nr6O3Br+wMB0oDB/8Ct6e2R/O1oZF1biZKIjVBELwQiZLWptdpPDWCaXmE2CCwJycLCQmE+DQkJEcqQgNQHjtogevbsWaE8qThpuk1LSxPQlAoaNLRCs/T2JWDSJG5pf+UlRXj+IAK3PbciePcAeC7+O67YvAUn6+9ogKImYNbcx3auTv8JPJf8HcH2vZFydSmyoh2Qf+eyiADVCgxLUIENeY8CmKz0w6AfyxpftIKV5uci66Y7EnfMx03rdvDu8Vu4tfk+XFq10AxGbeAU+1vA9cs34dX1l7gx7hMk2A1FpvNSvAqzgyJ6HhBuLZtoNZmKq4HJoJ98k351GRWWHGgsV0eQhIWFCUhSDWoDX332E8A02Xp7eyM6OhoPHz6UTbM6Ks3s7Gzhw7QUYFZWlqMwOw33g44idO8QuC/8My5P+YEO6rEmDDUrTZVjrVrg8tQfwmPxXxG6dyDSvDYiN+E8Cm5flH2a2mAr1ZJ9aTnzYVaWlSAvOQKph9bgpvVXwqwqFKROENSiMNW0QcXp2elnuDnuU6RsG4dcz1Uoi1gORNrIwNQZmAUmA6bRYEk1yVJ1SUlJAlznz59vUEjWBiyhSR9nUFCQ8Gkyv1COon29sk9tRWwpwKSafHb7OmLPzYP3yg9xeeqPGg6Stc22Vi1wZdpb8Fn1EWIdp+Fp+DHk33ZSRtHKptk6lTYVZvVsJeY7gbQwuebl4rH3OUQsGgLvnv9XZWZVAzitZlcDznH9/Dvw7vEbhM/ujEeO8/AyZBUUUXMA5hpqgkaz3icF/XibLA/TKLBk8A7NejSHMiDHmObW2lDU9p7mWU9PTxFdS1jKwLR0YCpQUpiDjPALCNkzCC5zfmU0c6vOClMJT5pnGSgUumcAHly3R16qs2yW1aowz6D4cZByei/zA6aiohxFGfdw99RWBI35VPgWXT4zhrlVf6VJv2bQqA9xe9ckFASsRmXkAhmYGn8QEJhLqmY6YS3ZBv6rNyxLS0uF2ZWKjpGs2mBmiv2E9dWrV4VZluklMjAtFJg06+c9xb3Ag/Bf3xaXp/3YdGqytrqU3lNl2ryFwI3tkOa9GS9SLiE/9Yycn6kJmlSYApiZYoq0Bv5O07n5yrJSFKQlIGnXYvj0eQcurXUI2DFAOeqjRmn29e37DhLWDccL79WoiLSV/ZgagclKPwSmspZsAxYuqBcsCUrCiJGpDe2bNASyNMsyTYV+TBmYlgZMgjITd3x2wWflR3Ce/D2jBO/oqybrOt558vfhu/pj3PXcgBcpskm2TlOsBFEBzBsoLzYPYFaWlohcyYSts+HV9VdoLDWpFqStWsK7+68Ru2IQnlcDUzbJajRJxy4CsvyrTLINBEyDYVleXi4UpZ+fn1mCUoIr8zeZmykrTO2wpD9T8mE2di3Z0qJcpPntg9ey9+A06btmBUoJoLwv+k/TfLYoTbKnZYUpwVHdOuUMXj5ufGBKijJh2xx4dv65ESJc9Te5qoWkqmpt1QJeXX+BmOUD8MLPDpWRjJSVgakZmJxAOlA5H6bx05YMgiWjJ+mjZGEBc1SUEiilNRVmTEyMDEw9o2QbB5jMnyzCwxBHeK/4wGxBqQpMn1Uf4+GN/chLvSRHyaqDpOq2Rgam8FE+SkPKvuVKRdkAoFOFXn1et2ohChzErx2OwhsboOBcmRpNkkYqAGCp1+B8mKyQlN0wwNQblkwPYdRrcHAwTB3xKsHPkPW1a9dEpG7taFD5vXrF2VgKs7K8BNkpAfCz+9zsQVkNTOvvImDjV3ga6Yj8FLlUnkaTLOvvVgPTtGkljHotycnE/XO74dv3T+Zleq0Lqq1awrf373B712SURKyTixdoA7kApi2QfR0ozQMUxlOYesOypKQEcXFxopKOIdBqrHMY9MNcTBYvkP2X6gFZ+4eDVLjAVAqTdV0LM+8gZO8Qs/NRSmCsa+00+XuIODIWOQkX5ZQSVSWp7nUjAbO8uBCZgVdwY/znulXfqQtgJt7OoJ/Aoe8i49xyVEQvl82xOgGTCjMIKDMeMPWCJb8009PTRT3XxkwPMRS4TCthRSE+gwxM3YH58uVLNDgwWTmlKBfJLutxedpbjR/1KkW/6rG+OvNtpLqtw4tkzmTiKPsv1YFSdVt1lCwVZsNW+qH5NS81BlHLR8Pty++Zp59SA4Td2/4AoVPbIf/6NiBqpmyO1RmY143mw9QLlixhFxgYCELHUGA19nks2s4ZTmRY6gZLqk1TKExWT8lO9ofXsn+ZZTBPXYqy5vYW8FvXGk8jTiIv5azsv1QFY12vReGCoAYtjUfXUUluFu6d3i5K12kNrtEArUY7lwE/Xd5G8lYrlMdultWlNlhyf7UPUwJm/SYo1xmWVBacLoul5RobePW5PhUxU11YgL2uGUsYOcultlmyOb/nDC9UmPziMfYffUkvn2cg8sQkOE36HwuG5RtwnvJ9xJ6didzE82rNsXlJp5CXdFIsGn17dcGlKW4XwLyuBKbxfEzSOGX067MIf4RM62RR5tfaYBbm2G/exTPPrUD0XFld6gTM8SpBP/Urvq4zLKkq6fPjbCH1gZU5nMt0Es6dqU5dcoaU+Pj46vzM5gzI2s/eUMCsKH2Jp3FucLP9o0WDUlKZ3svfR0boYeTVCvYhJJ9GHsbt67vwKOyADExV8FfXkuV8mEYEJgtbPHuK24fXwrOTmaaJ6KFkPTr8BHFrR6A8fpsMS11gGT4BCCMwmVYSALDSj4E/+HWCJdVEamqqUSr0ODg4gIs6aEr76tqv7hxDtqmqy9rAJES3b9+OWbNmibk3awOjub83NjA5toqfZyDqlI3FRL9KUKxrzeLucedmIzfhbA11SVj6n1+N5bMG48L+eciJPy77NWsAk7VkA1H+MsdoFgz6KnMiAxBq08UsKvTUVov6vqe6DBj8d+T628u+S51gyXQaApNpJSxcoASmZHbQY60TLBkBS9NlXb5KmmZZXk5KJeF0XJzUWSp/x/O4n3VjfX19xUTOPIbVdaRjCD2+Zu4mjyXQ9J3zUgKnttxPwpi+SyrI2qZYzlyyfv16TJgwQRRkl+BIs2xKSgpu3LghJqFmwXgGCnE7I2y5sD0GEDFamPt4LtfcfvPmTVE7l/5SToBNBcsfIKrHSe+lazE9h2UEeQ6PZ3tcE+hSe4mJibh79664j3v37olrcJJsntNQpmRjApO+ymep1+G1/IMmoSoJ0IsTW4i5NZ+EHUW+8F1WFVsnLD0dlmP2xF44tXMmcuKOVcOS4Lx7cw+i3DcjwXc7smKOCOX5IvEEnkQdRnbsUST670C8z3ZkRh2uNuVmxhxBkv9OxHlvw52g3XgUfhDPYo+K47nvhdLky/Mzo6ve5yYcR9rNvQh33STOk67F++MxvL7UXkb4AeTEH8PzxBN4GHYAsd7bkOC7A0+V92B0UzJNso84vRcVZv1N/mX5z5EmfJX/B5dPzTinUmd12QJUl8k7pqAy3g4Ia+a5lfoAk8fGLQay/KoUph6g5KFaYckByxk8mNivLgKW4GHhckKGZloew6o+/OLmFzoBJgXVEJQ8hsezPX7xE6A8hu0QoDyXxxOYBBMnepb28xhtC0HJAgQ8X4KnujUBThjVBoo6WLJcHsE+depUjB49GkOGDMHYsWMF+AlQKtH58+dj0qRJ6NevH6ysrMRz3b59W/h4ed7IkSPRv39/sdjb24sfBbNnzxbPyHtgX82cOVPcE1+zPV5j8ODBsLa2BvNECVP+gGB7I0aMENfivRw4cAC3bt3Ct99+K64zfvx4jBs3Dm5ubqLUnwR8Y66NBUwWSb/tuQ2csLkupWaJ26/N+iXuuK/D8ySqy6rIWMLIw2E5Zk3oiZM7Z1TD8nHkITgfscWkUV0w7puOGNG/HdbZDhdwSr2+C/Mm98GyWYMx/puO6NvlU2xeOlqYcu8F78XONRMwaVRXjBr4Ffp1/QxzrXvD7dRSnNkzB3vXWSM9dL8A6Pn987Bt5Tg8CN2Ha8cXY8LQrzFx2NcY3PNzrF84EmyLC9ueNLKLaI/XWjitP4Kc18H7zEosnj4QE4Z9DesRnbFx8Sgk++9sGFNy6tkqYBbXE5gKhaj9Gr1yrEVGwNalOl2/+C6Crb/Eq+h9QMQk2RyrLzDjpeLr+k3vpRWWDOwhfDSpNYKJxxA+VIdUV4ygpKKi2nR3dxdqil/4nKyZ7wlLQoiwpAr18fERrwlLvmcbhClVGr/02S7P5TV4DM/nuTyP4CNguZ3RutK1eR1NPlZCmypPVV2qgyVhxuAmXo9Kj+cRYnv27EFUVJQw2Q4dOhSHDh0S1yM07ezsxP2sW7cO8+bNE8+wdetWAVo+H9vo27eveCa2z+fo1auXADCVI++dCpF9RvjyXP7YWLZsGZYvXy5+RPAahCKfnT8ICFf+4KA65zE2NjYNakqWgGloWomishIFT1MQuu8bOFm3bFKwZCm8iCOj8SzWoXpmkrpgSSUY77MN1y+tRbjbJgHSb3p/iZCr6xHrvRUdv/wXbKf1g//5VQKAowe1h8/ZlXA5sQRzJ/XFKfuZcD2xBDMn9MS6hSMQenUDdq2diCUzBuLujT24H7IPe+ysMNuqNx6E7heqMfDCGsT5bBPnDu71BYIvrxdtTBvTHRcPzIfL8SWYPLILtq8aj4Dzq7HOdgTWLhiOG852cD5si/mT++LE9hnIjT9RrY6NqjIlYNZDYTKw52nAZQSNbWWkiZvNRJm2agmfHr9BlttWKKJmy5GxOsNSaZJl2cD4pcrZSnQHplZYslg6IaQJOgQiJ3tmDVaCja9plqSpkmDglzdf01zLYwhCCZY079JkSDDwWMKR4KCqpHmRio5tEoKECM9lG5wOjPAkxHguIcdzqFz5Jc79BI06NSwpTUKG19MGSyoyKmXe1759+7B27Vq0adNGAJHPSkW4cOFCcW+8jyVLlmDx4sUCaCtXrsSiRYsE5Pfv349Ro0ZVA5dwJDQJSz5L9+7dBRCpSAnGgwcPYtWqVWI7IUmVvWDBAnF9PisV7ZQpU8RzEpzt2rUTMOX9UV327NlTPB/bN6aqVG2LfV1cXGxQHqaULsL6qpaoHrXds//6L/H41qFqU2xdsKSp9d7NvUIR7l5nhflT+qJjm/fh5bgcsV5b0bX9R7h4cL4wfV6/ZCdU39VjiwTUZk3shcuHbRHkZIdFNgOwc/V4cc7O1ROEKrxzY7dQjPZrJmLG+B5Crabf2g/3U8uwb701Fk7rh7afvQsvxxWivcmjusDTcTmCLq0F2z64eQquHF2IsUM6YMyg9ti0ZBSWzhyEYX3bYMOikcJcbFRIqvow6wnMsoIXSHPcBu+ev7O4vMq6VKW03f2rHyHVfgYqY1fKsNQLlqrAXAY81X0+TK2wfP78uVBUmqDDfQQWYUblR5gRIgSR5Fsj5Kh6uI0gIywJKapHHi8pRcKPUCL0eCxBTeixbQme9NPRJEmI0vxIGPA94cd26Afk+ZrUMIFJyPN81SAfdcqS7RHqNH1Sse3YsQOdOnUS0OL5DAZavXq1UJm8NwKSsOT98TWhNW3aNGGm3bx5M3gMIVkblt26dRPPzvN4PKG7adMmYbolLLmdsKSpl6bYyZMnY+fOnQK+vK8uXboIgO7duxc09VLpUjmrwq0hXhuah1n2Mg8Pgo7h2qxfNElYui14B/d8t+BFclUJvLpg+SBkH45vn4GpY7ph64qxQsF1bf9vuJ9aKsDX8+uP4em4TJhtw1w2YczgDrh8ZCEITL6m6ZYgXDClnwAsTbeEpe3U/sKPSfOqgOW4HkgJ3IWTO2ZgwtCOQn0SeB2++JfwpbocX4whvb4QZtbp47qL86lmrx1bLGA5fVwPHN4yTSxHtkwTwUq5DR2kVA9gFmfcQ/zm6XD/6sdNDpY0xYbP6Y7S2O1AuHmaYivDJqA8dAK4Nr+atrwnayB+OZCpGzC1wpKmUilwR1Jk6tY0GxJohB1Nk4QBoUiIEQ7cT1jSNCvBkpCSTK1M5yDsJFjyfJp2CT3CkudxH82gVFVUqwQjX+fk5Ijr8D5prmWbvJ4mwPMZqJapWPmMEkQIS5pOqQD5HPRJ8v6PHDmCjh07iufiOQMGDFALSypoCZZU0oTbmDFjRJtUpTyXqps/CHr06CHugUDbvXu3UIZ8XoKO1+ezsB+oEglDvqeKpVmW90jlyf5gP1Fl8p4IVH4OvHduV3026RkbYm0IMF+9eIJEp+VwnvIDs4HlxQlvGO1ertj8FEnOi5CbcEb4LSVYEjoHN03B/ZC9eBxxSATa0CdJnyDNsOf2zUXPr//zOizjj0OCJU2hhJ7V8M5YMn0g7NdMEPCkkmS7NMPOGN8TEW6bEeG6GfMm9RFm1WiPLZg3uS/mTuojAoYuHVqAzu0+FErz+PbpGDO4PVbMHizao2mXgUBUrWzLbsFwxHhuAa9xm8FEpkp/ITAz9Av6YfrJi8QwhM/vD0aQSoqsMdfXPn0DXIxxD66tWyJw2L9QFL4PivApdcKIoCoIHINHrsOQfm0oXviNRlnIeDz1GIHQ433w1GN4nefqA7iK0PF4eWMcSm6OQ+WtCcj2GgHnLZ2xy/YL3DzS23yBGWENJKzQCZhaYUlQaDLBSuAk7AgaApBKke/5Rc7gIAKTX/S1YUlFSNhxP4+lmVGCpYeHhwAk93M72+S9ECYECGFIsy0hSgVK4BBAvAZhyfYIK23AZHuEl6QuCRpGw9KkOX36dOFv3Lhxo2iHanLNmjViP0FHlUgzMc2sPIZw4v1v2LBBKE1CnuZZHktzKdsj9HifVN4M3KFaJVzp5+RxVLDs02HDhmHp0qXieoMGDRImX/qGqWL79Okj2qNPkvfD/uC1GMFLmPK++Axsh33REHBU16ZewFQoUJh1FxFHx8PJ2rBCBE7W3xGBQQwOcpr0Jq7O+Blc5vxGFAaoNpFatcSVaW/Bdf7vBJRZ9MB13m/hvugvcF/4J1yb+fPqlBVuZwUhtnNpYot6Q9N50vcQfdIaz2JPCb8lYel7bpUIrhnW90ssmNIXdrbDcWybjfA10tTJIBzCb0D31vA+s0JEpg7t8yV8z64UqSaR7t8KBUqQEaojB7QDfZj0R1JZntg+HQk+24VJle3Nse4tzKaE4LxJfZHotwMbl4zCsH5tsWedNZbNHIRenT6Bz5mVOLt3Lgb1+Bzjh3YU7c2f0g+Ou2cjzHWTOHbKqK5Yu2AYdqwej/0bJ+OWy8aGCfBRNcUqXxcogVkhfJja8zAVnBkp2AM3rdoa7K90/eJNkZvp1ub7YLk5726/hufXP6sBX3FM57fh1fWX4GuqWN++f4Rf/z/Dp/fvlaq2hSiGEDDkX/Ab9A/jBBtxRpIev0VuwG4oIqarNcWWh45H7OkB2DHvc8wb9QEWjP4A385qhRtHeotlzZT/IOxE33rDUhE2EQ+uDcWFjV/j+qFeKLw+Bn77e2Lq4H/i0PJ2iHToLwCqD3xNd+wEgMCMX6HVJKsVljQzagMOv5QZZEPVRGXFL3wClkCjMmMbfE+zKM2xNNkSngQdg2aoEqk2CUiuqY6oGkNDQ4VCItDo0+M+LmyDPj1uI6wITUKV15H8p4QeIaIN9LwW71HyW9LUy/MYUEOFR8XH5yOs6XPctm0bjh49KhbeA0HP6/MeCWz6Nvmaz8JnmzNnjoAp+4cBQVSFjFqVziNYqQq5/8SJE8J/S+jyGtx++PBhHD9+XDwr4T9jxgwBXB5PczBNsTyOPxiozHl/W7ZsEdfifZhKWUrw1HW2EtYCzXsYgxs7ehoMJiq3G9t7IGhbd7gt+CPCDo5Csss6BGxoB4KUwCNIb+0fhjveO+Gz8mNh8k1wXob4C4sQe24+4i7Y4vqWLrg6/f8hYGM7RJ6YDM9l/zT4nrrkuMAAACAASURBVKohPfENOFm1xK29g5AVeQz5qWcFWFiQgAE0+zZMEurSwX4WAi6uwU1nOxzcNBmHNk/BxQPzxDE0p2ZEHMSlgwtE9OuLxJOgv5GBPUzhOLBxsgjqObR5qjDLEmRUjARySoA9zu2dix2rJ+DEjhnC7+h1eoXwezJlZO96a3F9tk1IJvntxG47KzDAhwqTZt7lswdjwdR+4v4YaMT2eAxByfOSA+zBikQN5rOsBc0CkVbCPExGyWquJUt/eIaHIwKGfWCwkiP0YtdaIeCbfyFozGdIPbAScRunwqfPH0Sbrq2/g4Ah/0SSvS0St82Fb78/46b1V7h7YhMSt89D4s4FSNg8E/6D/gHXL7+HWLtJiFo+Eh4dflp/hdmKKSQ/xZPLm1AZOec1WBJgd52HgEC0HfMhTq5pD6dvO+P4qq/gtac73Hd1w4xh/0LAgV5QhE8EwVpwfQzyAkajJHgcFErTKdfFN8YiP2CMcvtEcTyPoWItDhoLqsrH7sNFm+Gn+iHHZySOrfoK1gP+gYQLg5AfMFrA8tXNcXjhPxovb4ytbp/Kt+LWBJQEj0dR0Fihek0HSintRjLJUmH6VE0grSatRCssCUBdYEmg8Muc4CAI+Z7goiok+Pie7fALnaqTEOOx/NLncXzN86meqA65n8cSZjyex3AfISTBlNsJJB7La7INvub1eBz3a7p3pqGwTQYYUQHzC59ql6XwCDNClGtCkNBh4A1Bxu1Uo1RtPJ6A5Dl8LZ3Pbbwf+hapPPmchC9NqoQm/aA8hz8YuPB4gprXYRu8FoHPa3E7j+VzEY5Up2yPQKciZeoI74XH8Fmo8LnmewliplzrAszKijLk3g0Gg2BUAaPPa6ZnRDnYIOrkVPis/DcSL69CZoIX4s4vwNXp/ysUq+eSf+BewEFRd/bGth6iStCjsHMIPzwW17d0FuemuG6E//o2CN41QLRxc2dfeK/4EG7zf1/v2U9ubOuCp2GHBSwJlecJJ0QuI82vGRGH8CTykMiLZM4l8xjpX2QeJfMieSzzJJkHydc8n8DMjjmKp1FHYL92IqaN7Q7CkmbZFXOGCLjddF4njmcuJNsTuZdxx/As9pgANtt6eGs/0m7uEfeSFXNUXGPr8rGgeiQs2d7i6QOwdMZA3Lq2sbo9mmV5Hu9fuidTwVJcR1T6kUrj1Q3MilfFeHBxH3z7/8VgMF0f+W/cO2uP4EntETa3Hx57nUOGuyNCpnYSKtKj/U8QYTsIT3wv4YHTQQHUyCXDkeHmgFszuuPWrJ4CsKkHV8Or22+QvGcpkuwXillPAoa+D89OPwOBa6hZ1q3tD3H/1HJURtlWqSOVQBeaWs+s74gpg97FpW874bnfaBBWz7xH4on7cHjv7SFg6b+/J3J9R8F1R1dhMqUKPbHqK2R5jRAQSzw3EEdWtMOeRV/CdWdXAcIMt2Fw+raT2HZmXUfcdhoszLyee7rj5tHeCDnWB5MHvYsurX8roEloJ18cjCPL22G37Rc4tKwtUi4OFmC84zwEnru7w2FtB3Gd1EuDBbhNDkzx40DyYaoHplZYUj1pAg4haKkLYUnYUpVJsDQmVOg3pDIk3BihyoUqkIqZQNT3WlTRVLcrVqwQbdEES+VraHv6Xl+f4yVgcqJwdX+V5aXITvEHJ07WB5Cqx16b/UtEO84U1X98Vv0HcecXIj3kNFLcvxXtXrF5Czft++Je4CGkhzji5o7ecLf9Ex6GOMBvzacChAzCSXHbhIhjExF2eAzS/A8g6aqdUJ1Rp6bBe+VHSpVqmC+TavVx6AHkpzLIx3gqjKCiL3HHqvEiKvXbpWOwZdkYXDm6SKhPfa/FogOBF9diy/IxIofy22VjwMXt5BLhm9S3vQY9ngrzsbL4emW5uuGF8qKCqkjYXr83HJajPsb9C3sQPLkjwucPwL0z9nh47bhQm17dfg2/AX9Dwta5eHDpAO6e2oqgcZ+DsLx/bpcAoVub7+HmxDbIDLyKoDGtkHJgFR66HEfi9rlI3r0YkYuHCdOuS6sWBgGT5uE7B+ajInrxa7CkKXTVpI+xePxHSDg3EFSaEoAYdON/oJeAJc2lVIUOdlWw2re4Dcb0+iuctnQWynHdtE+wevJ/cHJ1e7js6IpMzxG4sOlrLJv4b+xd/CUube4kwBd3diA2zfwMjnYdcfNIH0wa+C46t/qtAGPw0T6wm/Yp1tl8gtPrOmC59cdYb/Mpsr1GwmV7V4zu+RcsGvchDi9vB8KTKle6V5OutQBTKyxp6myqsCTkqdaoFhsCllSJ9GHSRMx+pEqn4jPUNMrz6LelP5bt0ZzN94a2pw/8DDmWwKyr+Donec5K9AbrqKoCUJ/XtWEZc3oWUtw2I8F5JcKPjIfHor8h/uISxJyejbu+e16H5aQ3hR8z9twCxJyZIwq5p/nvR7SDDUL2DELSNTuEHxmHK9N+bNA9MljIz641MoL3KSeFNh4sCSOqUZp1Q69tFHmSifWsrMNKPamB9iJPM/jKelEZiBV96GttUPgZ8iOCE0g/CUZFifr5CssK83Dn+MZ6zTJyvRYsUw+uQuqBVQJ0Nye2Rej0bkjeu0yYXG8fXf8aLF1atYR3j//DU39n3JrRA6mH1iD96lGEzemN2LUTkea4XahRQwOQXL98Eyn2M1ARvey14gTPfUdjzsj3sXbqf3D/6jc14KMKSypLgpXKL8KhHzx3d8PIHn/BCuuPkeM7CtOG/FPA8taJvnjqOVyYSgnUuSM/wJWtXYSvsjBwDKId+2PJ+I9wbOVXyHAfLhQk1WXKpcHw2dcDPdv8XihHKs89C79E9y9/h6QLg3B2XQf0bvt7EQxEEJcGj68BdpPCkspcAJN5mMuAnGBAZeo4GZYNCEtDANOUzmEOZlFRkdoczIqyEmEyrc+UXLVhGe0wEwRf2KGxuO2xFaH7hwuVGbCxPe742KuF5eVpP0b8xcWIdpwu1GWi8wr4rf1MBApFO0xH7Jk5uDbz7XrAshUybu5tEFiaHcAMgZ6B5+QlO6DooS8qXuUAagqvM8fyzrEN8O7+G4NUG02jtWGZsncJopaOEOZUKsqELXMQv8kGEbaDcfvIutdgyYhVn75/wtOAKwiZ0gkpB1Yiccd8eHZ+G/6D30Oa4w4ET/7a4IAfAcud06smhK5VySfPfzRsx36I5Vb/FmZSVeiowtJvX08REXt2Q0dsn9ca2+a0xoCv/4h5I98XyvLi5k5YMOYDrJ36Ca5u74Isr5EIPdYXdlM/EeqSJluaVCNO9ROwPLGqPZ75jMTRle0wa/i/8NC1ymTb+v1fCDVJUG6a8RmmDn4Xt52GCFha9/8HEs4PNJ+IWapwzury1BNQsVxohSUjN42tLNkefYyqPkVpG82iVHv0yVH5MShI8k3yHPokjWX2pRmWflH6BqUAn8aEFf2gDCSi/7IuMy23049J0zGPb8z7revavEcq9fz8fJSXl6ut8SmUZbIfvFf+2yAQUYG+DssZiDoxVQT40PR6P+gYYs7MhfvCv9SEZehpBG5sD5fZv4Kf3eei3F7I3iG4Rbi6bhQBQ97LPxDgpSmW/k99FK/qsQEb2uBxyP5Gh6XwU4YdEP5G+j3rAi2Po0+Sfs5G8UnqAE+CsvCBO8ry70NRUQrg9Rqy5UX5SDu1RRQkMDRd4zVY7lmM4MntEbtuEh65OiDt9A6EzuguirTfPloFy6glI/Dg4n749nlHqNqIhd8gw/20gGPKvhW4fcROBPwEW7cXsKRCZRStIX5Lmnlv75uLiuglrylL+ift53+OqUP+KSJTpZQOBuQwfYRqjwE+9BcGHuyFUT3/igNL28Ldvhsm9P2bUKUMxOGxVWbUTzC+799w63hfvPAbhcTzA4XZlEE8x1a2Q+ChXmph+chtOK5t74KOn/4GZ9fTRNtbLOEn+qEgYAwubOwoomaTLw6qDvpRBbvpX1tVgTL9NPAqs8YMJVphyUhVYwKKbTHNgZGrTJ+QoEhA0rzIbYx4pZmRgT4MEJIKG7DgAQNyjAlLtscAm4Yww9YFk7q28/kYHcvnr8u0ykAe9glzLNlXdUG1rms09HZdQEknEwN8nt25IcyUqnDR5/W1WW8j4piVMLmyEHvE0Ym4dWCk8EvGX1yEjIhLoKq8NvvXSL5mJ6Je3eb/AQ9uHEe0wwxxbILTMvBYr6XvIWhrN9z23C6UZtTJaUh0Xomgbd3qFeQTtKUTnoQdqg7wqQtSDb39ceRhEUV7fNt0ETxU1/UYuHN0qw08HJfjSeThOqFa1/kNvV2A8r4byvIfKEGp1mWJipdFuH9+N3z7/slgn2XgsPdx5/gG3JjQBrdm9kDClpkIHPEhgid3QPqVI2Dgju+Av4j0FPogr4/8DyIWDMKjaycQtWwUYlZPwJ0Tm8XavcP/ImHrHKQ7HUTMWisk7Vosgn38Bv7N4NlQ3Nr+APeOLUFl1MLXfJbMdaTv0GbIe1g16T/w3tNdQM9tZ1cEHe4Nj93dMXvE+/DY1V0Acni3v8BjVzcBMqv+fxcRtAz8CT3WBzGnBwjf5Kgef4H33u6IdOgn0kF89/XE3JHvw37BFwLIVLGn1rQXQUCMup076n1keo1AlGN/8FxClf7TmDMDEHtmAF7dGItLm7/G9KHvCXOtFIFrekBK/ty6QclRphWWTNMwJiwZ1UpQUj3xi59ApKrkdejPk0rXUTkxopXpD/T1MYiFiopAMabSJXjoV9QXloQCF0KNi/SeUa61QScdp24f4cXtXPgjgvViqeb5nvvYlup57DcWN2AKCfuGbTc0AHVtn/eiqijVf41VbWXqyIsHUQja0sVg1XZ56o/gt7Y1fNe2EmZTvmagz5WpPxbRsfQ7Xp35M9DUykhYz8V/Fyox4ugExJ6dK0ystw4Mh8eiv8J50ptifWNHb4QdGiOiZAlP5mHqA/Aax1q1RMiuvshUpo7oAxJpkmiqO8lnSEWo+l5qj/u5XdM+zmjCHM4ZE3qKCFyeq+48RruyaAJTS1hTVrqGOayrQOlaBcrKMk3DC5Wlr/DI5QT8h7xnkGqj0vPu/mvcmtULvn3+CP9B7woo0qzr1fVXwl95Y/wXcGv3Q/j2+5OIkKV/MnD4RyKNJHHbPMRvmo6Q6d3g3uEnIuqVKjJ8wUDErZ+M2HWTxbEEniGq0uWzFvD46i08urgelZGcCNq6hl+SwKEv8fKWzpgz4n3YfPNPzBv1vvBFutl3BX2QNIlGOfRH8oVBwqTKoJ1vZ7fConEf4fCytiIilqZZmmBp0t088zMknh8kAnzW2XwqzqFvk35P+h9pevXZ212kinju6oZ9i79EfuBooUSZumIz5J9gwJDdtE8EOGkqZl6m/YLP8dBlWOP6KlkvlqZXoSif1lCU0kDTCktCy5iwpJJk6gcByC9+wpKmVoKQoORr5mcycIWl7vie5leCgfdibFhSwTI1Qx8zLI+nuZTRqQQb75OmXCpCCe4EMOHBHwS8bx7D55bq5EoA5T7eA3M7mU/JEnhsk+cxZ5V9wh8NXLNNXtscYcnnYR9KpldpgNW15mw2BU9TcevgSFyyqk8RdRYPkBa2IxUTaCHyHKvhZVWVd8n3TlbfgZP1d5XFEKTjGe3aApesuLTEJZXjq9uYqF9ELIupRx4bh2cxVUUJdAEO68SyMg5zGON9t4sI1WjPLSLVg9VzgpzWisICTC0h7BiAw8CeG052IjqWRQekoBxpui1GzTLHcs38YZg+vgqWuQknRHDQzcvrxLlJ/jtEaglNsOYIy7xkRxTed1GCUn2EtepYU5SXI/P6NZGmYRiMWGmnqpiAWNd4/YYoMsAAnqq2ax7n2vp/4Pr5dwUgr6lOvcXjW7WoShdpzXMNi4IV12zVQkD7mY89FBEzXsuzlNQZ8yHTXYYi9HhfkdLBYB+aV7kwH5JrpplkuA0X+wlDbs/1GSnyJ7mdZljClf5KHvvcd5RQm9yedoXtVVXu4XbmY1aEThAmVka7MoeSkbg8hlD2P9hLtPXUcwTKb40XwUU8rjR43Guwl56h4dcE5RyNoOTY0gpLAo3AMqbpk20xJUWCJfMrCQEWMyBMpdxHgoU+RYKV4GDOI8FirHuhQuU1CSapgo8uKorRrSxjx5QQVuEZPny4qJjD+rDMo2QlHfpDmStJgLJ4AGcHYQm7iRMngrVb+eOASpoFClith/s5a0iHDh0EcPljgWknrMjDa7BWLIsWMHLX3GCpLyilL7WXzx8h7tz8epk5DQWZKc6jok24OBe5CadrTAKtCZrMjWRBABZFXzX3G4wb0lFMmUXTKGcRYbUeVvhhJR/C0O/cKlGebuKwTqJGLIuc+19YLdQjIblk+gCMG9JB1Hsd2reNyMvMCD8ogGtnO0KAkTVpl88egnDXzUgJtDdDWBKUSkWpJphHGk+qa85okxsbLHIdXeqRy2g4aI1T1q6u6zN4yH/Q31AQug+KiKmNCBrJhGmpayUoHzgCr9QrSmlcaYXls2fPRMCNMU2fbKs2LJn4XxuWfE+QUnXRdEvAcC35OesLTf4IoPlXF0CqHkOF+MUXX4g5JxmQxDJ27du3FzOEsE2CjdNy0QfLYuYsacd8SD4Hy9CxXB1NrixawHqzhB8VKav7cDYT/iDgjwTOgcm8TCpL5lbyOjyPRQ3MxQxrKCg5AEuLckVKR30CaEwBPUOv4TL3N7jruQEvkjmnpYNOJk0G1uyys8KXn/5DzPjhd24lWEi9V+dPcG7fPFHibmDP1jizZ7YAJmcoYcF0D4dlokoPK/Cw8g6VJnMmrYZ3wrVji3B692z06fIpJo3sirQbe7B7nTU4NRfL3BG4hPCutVailiyLqJuPGVYFlJXay9xJX2xcFz5IFf5BtzaGmjobFnh1gVCX7Uw3CZ3aHiUxu8y2kHrDK0IjADp6NvDgNPCyZjCP6jiSXmuFJfPk+OVtbFMsQcgKM4QhAUMTI02sNLnSDEnVRYBwH+HIiFgGtFBx8Zz6gpLns01Wu9HHBEtoEpY0lxJ2fAbCjoqR76kmWSeWNVzZPgsHsIYrFSF9j3wmzn3JsnSEHuvBMsCI7TLIqXfv3uIY1nfl7CKcoosVemxtbYX6pGIleM0BlgQlFTlNr3UVH5AGmrp1RelLPI13h+fivxnuF9TTNGoo+PQ9jzmW9J9mBOsXCUtYspzc2MEdhM8wK/qIKGFHIHIuShYx5wTMh76dCqfDC8AptQhCqkwG57AcHee1ZL1ZqlNOqcUKQPRFcmJnm7E9EO+zHVYjOotptqhYj2+zEeqTqpTVf2zMBJZVplfJR6kfKDneSl88EykdrNuqC4As6Rj6OhM3TURFnF2dJliLgJVK1SGT32/0LKXpVTsoOZ60wpIT+xJSxoIlwUdgSJCiL46AJChpZqVypMmV/kD6LmmC5T5JWfJejKUsCT2qVX2De3geA3Go+GjCpXpkoXRup0Im6AhLgp6mWr6WihHwRwLNsYx6Zbk6ApHPS1hSpVJ1sn9o0u3cubMol8c2uBCS7A+acRsblvUFJQefqA/7KA5B23uo+Br18wvqCzGTHW/VEqF7ByE7+kT1fJaazK/SPsKSNV+thn2Nh2H7kR1zRECPBc/5nj5F6xGdBSypLqeM7irmuqQPk2XyWPqO81Cy9iwnaObkzZxcmmXvOP2XzbgeiPHcihH92wnFumbeUNAcy0mjORUXa86ag7KsGcyjPyg5vipKXyHD4zRYWq6G71DVj2iJr1u1gGfnn+PxpfVQRM+XYWkIcKNmAulnXksP4bip608rLBmIQUVEoBlDzREgTAVhKTguhB8VFaNkGQRDHx8BSlVGKBKYjITlcTTd8jhj3AfhTzMpo0318VdKylIXWBJ+NLvSj0nIEaTsx4EDB+LUqVM4duyYMMOyP+i/JQQ5JyWVPF+PHj1a+G+5j58BF+ZWNrbPUgJlQUGBQYpSdTC+ys8ECwE4TzafabqMAVQWeU++sgzPE6um55JgqG2tDpaLbQaI2UFqw9L15FIBzn3rJwkVysLr6xeNgO20/rh8xFbUdV0ycxDuhewFC6HTZDt1dDdRmYcgnTWxJx6FHwBL3XFeSkbTmkeAj4NewTyq40n1tZimKykc4bYDa8wUYkkKUt290gR7feRHKIw4AETUPT2XyZWaIdAy9Tms0BM1A0g/C7zKUhv1qjqGVF9rhSUPZhUW+tuMoS5pRqwNO3Xbah9j7Pc05UrqlgDUZ6Gi5LRZ7BMqS851SX8itxOIVIz0WVJFEo4M4OEyf/58fPPNN+JYRs8S1pwYWtpHkyzVJM3QBCgDhQhMmnJpjqXZliqbZlnOZsLjCC597r2+x0qgpOmVVof6/glTbJwb3G3/2GRMsTTBsoxfRsgB5KVUTfysDZLSfsKSJlYqRkbFsmj60pmDRbAP36cF7xVTdB3ZaiOiZTl585DeX4BTcdF8St+jg/1MMekzVWbfLp+KgJ1pY7qh61cfCdWZHrpfqEjOY7lwan8xd+Xq+UOF3zPRbzvmWPUW6rYxUkf+qyjvQ6FSPcWgcaZQoCQnU5SZ8+z08yZjinVv9yPE2Y1CRcK3cmCPPrAlKCOnAw/P6Q1Kjj+dYMkvRX65G0vVGRt8hrRH9aZvyogEGvpNGbFL8ynhQZMxTafcTvVHoFEF08RLeHIffZSEHtUkFTUVLRUjgceJnGm6pYpmu/TfEsIMPmLhdIKSUbPMCWWbVNk8T7q+dF8NvVYFpSE+SnVfeDTFFmXdRej+ofVMITEf8y3TUiKPjkdOHFXlGZ0CeyRY5sQfF3NYBl5aA9Zqpdpj7VfOKcn3NLdev7QWib7bxWsqQc4QwmAeTvjsd3618F0yBYWmV+7btmIsOBXYDWc7hFxZL1QkZxrxP78a21aOFYXYOY0X004Yjct0knifbXgWd0yve5eewfA1FaWyMo+WPEp1Y0ndtsrSEmQGueDGuNYGz2upTt011jZGwfr2+T0yXTYDMcyvNEKQS3NooxqU55WmV/1/6OsESw7C3Nxc4ZMzZlSsIZAzxjnM7STgDIULoUGYcc02CD7V95LJlPulYwlNBv/QjMrjpWvzPG7nfr6u3Q6BLu1nMBHb4/m8hnR9qa2GXPNaUh6lsUApfbmVv8xHevApUb7OGCbQxm6DKvl+4C68SNZPVUqQkcyiecrSb8yJ5ML9LFYgTKaJLFZwSkzhxam3CDkG+RC21UUMkk6K3EnuIxxzE46LRboO2+E+ThMmpgETbZ6s0b50bMOvHVBUXcJOc8EBadzosqYbqTjjnigU4P7VWxavLt3b/RCRtr1QEmP/WtUeGZx1/HAQptfpwKMLyvQQ7Xm66saWzrBkjU+aLQkaYwCrsdog7Gk+JZz0jYJtSBiZc9sNCUoOSqrLwqepCN7Vv2YhATONdNUEYyfr/0HYoZHIjtdfVTY8kIw764nR7jeFoPRQqfWq7qvK8G2c25Izf1wf/alFq0vOfenX9x08vrgalbGLlDNk1AGI5qAWdXlGVVC+ZB6lYaDk6NMZljw4JyfH6GkkpoQmfaP0IdIMas5wMqd7UwUlfzA1zJ8C5a8K8ejWWbjO+71F+y452XT6jX0Gq0qjAUiHguTmca2GBSXHKwN9ip/cR/KeJfD8+v9ZrLr06PATxCwfhFeR24CIybIJVhssJdPro4vAyyf1AiXHkV6w5JclfWaMXjUl5Ix1LapKVgWir1DfCFhzApip7oWg1LXWa30hyoorrOgTfcrGYiv6XJ72I8SdnY3cxPM6V+wxD2A1luJ0RNEDT6WiLKnvENJ4PtNIciICcGtGd7AcXWP5HA29rtsXbyJo1EfIdtuASpEuIitKjWZnAUoboBqU9f+hrxcsORoLCwtFKoclmmNZNIDBMzIotUf/mhKU0rdcZXkpcu+GIGBDW2XdVvMJ2tFkeuU+1pvlDCWZ0Y7KvErdKvY0X1hSUSpBWd6woBTjS6EQRQrSnQ+JKbJYo9VQcJn6vKqgnj/g7v4ZKItiEYJJsqrUpColUD68oFSU9Qclx5DesGRkbFZWlsiJNFZxAGMpR03tSNV6CAEZltphaSpFKYGyaq1AeUkh0kMd4bH4b3CyZvFz8wemk1VLeC17D+nX9yIv5QLyk2VQav4R4ICidE+Ucj5KU4BSOcgUFRUoepRWZY7t8rZlwJIF0zv/HHGrhqAoeCMUkSyaLqvKOvtAMr1KoKw03EdZ87vJAFiyAZpjGbXJvEJj5F5qgpwx9jHlhWkYjCCVQakfKBlNaMo/Xq+kMAcpLuvhNv/3QrGZMzAJSnfbd5DqugYvUpyQn+Jo4nSLxjKhGnhdBvMQlHn3UFn+Su3EzQ053phKkpcciZhV4+De3vyjY+mnjJjTBblea6GIXiCDUtMPhWpFeR6oZzCPujGot7KUGikpKREpDSzxZs7AJCiZ/M+0CxmU5g1KaWxBoUDRs/uIO28L13n/Z7YKk8qXQI+/MA/PU5yRn6pfTqVm9WUgjMw5sKeRQSmNr4pXRciJDESE7WAwFcPUZlVdr0eY35r2FTKvLEdF1FK1c1bWqbA0QaVJ7mPBARvgISvz1C/qVRontdcGw5INscg667kymd4cgSmDUjscVYOFVE2vplaUtQcmIxgLniQj9uw8uC34g9kBkykizKeMOzsTOQkXkZ/KnMomCDijPVNV1Gtp/j0oKuijNK3Fosb4UihQXlyI7FBvhM8fAPevfmRmwGwBKspbNu3x5NJSlEUul6NftQFelLCTar3qX3Cgxvio4029YMk2i4uLRc4iK+KYkw+TKSKcd5KKUs6n1A5NcwKlNFYZIZv/JAnxF5fAc8nf4Tz5TbPwYTpP/h68lr6LhAtzkJNwHvmp52VQaoRqbVBKn3AjrgnMl0V4FuaHyMVD4fH1/9ZvMmZjFWRv1VIUSQ+f3RmZzkpQRsr1XzUqaFEU/bRBJez0GYH1hiUv9urVK+HDZFoGIWUMP6OhbTA9hAXMOUsJxm90VQAACx1JREFUK93IplfLBKU0iKkwC7PTkOq+GX52rXFl2lu4ZNWicaBp1QJXbH4C/3WfI+XqMjxPuiQrSo2QpNJ2QOED96pgHqEopU/WPNZMKXkeH4q4DVPg0/udRi24zgLpLGUXs3wAsl1XoDxymawotSlKMc0WJ27WbZqt+ow6o8CSN1BWVobMzExRt5T1TRtDZdLsyqAj1rGlmpRBqRmUqukh/Pwa2/Ra90BWoCQ/C+khjgjePaDKjznpuyYFpvOkN+E2/3cI2dUX9/2340XqZeSnnpUVpUZYstarmxKUpXV/vI28R1FRjqJHd3H76DoEjW0Fj/Y/gUurlqYzzbZqAY/2byFo1IdI3TEBef5rURG9WE4R0QhKK0BM3OxgElByiBoNlmyMaSWctol+TE63xZk9TOHL5LRfBDTNrqylSmjLoNQdlIxuNl9Q/veblDOUMA8z/uJi+K1tjaszftbgvkz6Jq/N/Dn87Foh7txMZEaeRMGdy8hPlqNeNftoCUrXKlBWmi8opdHF8V9WmIcnfk6IWjYSfgP+Cve2P2xgaLaAW5vvw7fvO4hc0B0Z5xbhVZgdFFFygXSNZtdwgnIO8ICgZDCPafzfRoWlNPCoUp49e4aYmBhRHq8hoElzK5UkIcmpsDjjB/1uVJRUTKqBK/LrmuBUVZSWAkppbPEfgyrzcdRlRJ6YDJ9VHwuYOVNpGss8a9UCVJLXZr0N39X/RuTRMXgYtAd5qVdQcFv2T2qGpNL0es+lqjKPkWYPqf78G/gF/eTMxbx31h5hc/vBr/9f4f7Vj8G6rLpGsWo9rnVLEYXr2+cPuGXTDmkHbFAQtAkV0TS7TpXTQ3RWlKYDJYddg8BSGs8SNGkW5ZRV9CWy8g/VJmGnr1+S51BFEr6croqTRHOaKgKSapJQlEFZE4y1fyiwf9hfnI+SoLTUPxZff5X3FI+jryDacYao+uNm+47wKRJ0l6xa6mGmbSGO53lXp/9U5E0GrP8C0Ses8OjmAby4fRUFd5yUxQZOyqZXbabXe9eUoLTc8VVZXobC9BTcv7AXEYuHInD4R/Dq9isBOfoW9asA1ELAliqSBQYChvwd4bO/xr0js5F/YysqYtdUTUisERJyIQIIRTkbeHBKmUdpGkUpfUc2KCyli3BKJ5bJYyEDzsVIcLL0HGvMMiCICpEQpJ9TdeE2wpVwvHz5spgijCqShdDZFisJSZCsDQX5/evQbCqglMYV1wwAKnuZjxcPopDmtxfhR8YhYEM7eC55F67zfotrM3+GKzZv4fLUH8J5yvdF3Vmu+Z7buZ/HMdqWgAw7OAx3PNYjO+4i8u+6oeDOJbkij0Y4qqbLOKBQAmU9ZndQ/Xwb+zV/lHEC6ewQL6QcWImw+f1xffQn8O33ZwFPz6//V6SeuLX9gTCpurX5Htzb/kBsY/qHV9e34dv7dwgc9h7CbL5C8pbxyHLdiFfRu1AZt6oqN1CGpG5qWvgoGweUHIcmgaXqgKdfk/mZ2dnZoqB5fHy8qDXLwgGMpuWkxlzo86RyZFQrj+E8kPRDSoCUfZKvw1DTD4SmCErVccXX9DuVlxajKPseshJ9cC/wABIuLUX4kfEI2TMIN7b3QODmjgja2gnB9r0EGOPPzcIdr014HO6A56luKLjvicK7l+VC6DoDUoJlFShFCTtFw+S51f68TfpeoQDL5ZXlP0d+aqzwbaY5bEXC1lmIWjYCYfP6IXR6Z4RObYvwmR2EDzJ+7VCkHZyFJ87rkXdzH0oT9kGRuKEqMCXcWp5iS58fCYx6paI0oY+y9vgyOSxr3wDf80uOEKVZkAFC9HdKC6FKX6QER9nMqh8kCVD2GfuPpldjT9ys7vM0t21UBxVlr1BanIuiRyEoTPdDUbovitJ9UHjPtcrEKlffqYd52QGFadeqgnmaIig1DejKSijKy1DxqhDlOYkov30cFcl7UZmyC4qUb4H4ZUDU7KrIVpZjC+cim1T16gPmUTYyKDkEzAKW0lgkNPPy8qrBqEkpyft0g2ZzB6U0tlgxpqI0HwVpV5CfJKkhea09WEdzH+Uxj/JeMwXlfwcXwBzSnNAqMAooykDUC4hqf0BMqPLlmjjqVfVjVX0twzJDN+hYIpwlFU613hwVpepAl2GpGXqGQFMGpcoIk2FpXMUszR7ywDQFB1Q+yTpfyrBsorCUQVl7zMvK0hAg1n0OFaWL0vRqvGmQan9qFvNehqXxYCnNHpIu1Xo1bdRrXWNOhmUThKVselU33GVY1g0+fVUnQSkVHLDc9BB1o8TgbTIsjQPLalBy9hCWsDOfYDEZlk0MljIo6/q6+y8s8+izTDrZvJdkQ/NFpRJ296CwsIIDdY0Mo2yvhuUsIGx8VaQrv/ib66LWB6nFj1sNynMNMh9lfT9ns4NlUVERcnJyRGqJFBHb3NYsGmCIj5SglAoOyD7K2v8aClSWFaP4STCK0r1RlO7V/JYHXii87y5q2uYlnTAg+lUVlOZfwq72CGjQ9/zhkJ8M3NkLMAq2OS7Jm4A41rS10j8tRgLlQ07c/AQwwzxds4IlBzNTSPhFzzSS5ro8f/5c74hgVVBacmWeBv1CU1SisrwYlWWFzXMpLUB5cab4kaA/LAlKzh7C+ShlUL4+ThVAZQlQmguUPGuey8vHwBNXgKkehJ+u6tICQMnP2+xg+fogbF5bpPQZfdSlDMrmNUYMf1qq60IUP/SDfrB0QFE1KDlxs/wn94CaHuCPhezAqiLnusKyBigfA5Xm6wOXYanmM2/MTfrCkqBUnbi5Me9dvra594AhsFRO3Jx3D5UVr8z9AeX7a8we0BeWApTTAGF6NW9QsltlWDbm4FJzbX1gKYNSTQfKmzT0gL6wrA1K8wjh1/CA8q7G7AF9YFkNSgbzmD8o2a0yLBtzcKm5tj6wpKJkxSP6KHme/Cf3gOYe0AeWKqAsp6KUx5fmvpX3Cp+tLmZYCZTpZ5XBPOZrelX9VGVYqvaGGbzWFZYSKDkNmgxKM/jgLOIWdIUlQemO0rw0VMqgtIhP1ixuUidlOQGInAaknzbbqNe6+lKGZV0900jbdYGl5KOUQdlIH5LFXlYXWMqgtNiPt7FvXCssCUobIN3RLPMotXWfDEttPWTi/dpgKYPSxB9Ik7qcNlgqo15lRdmkPnWTPYw2WEZOB0RRdPOqzKNr/8iw1LWnTHScJljKoDTRh9BkL6MZloX33ZSmVzk9pMkOgYZ8ME2wjJoB3Od8lASlZfq/ZVg25OAxoG11sKwd9cpj5D+5B/TvAfWwzEs+hcL7rgKUCpZtk//kHjCkB+qCJYsU3D9p0aBkd8iwNGRQNOA5tWFZG5QNeGm56SbfA6/DsmqaLReU5smVeZr8x9/QD6gOlgKUJ5Q+Ssv+kS/DsqEHkJ7tS7B8/PgxZFDq2Xny4Vp6oCYshaLkNFt596GoKNNyrrxb7gEtPVADlhOBqFnA/aYBSj65DEstn7+pd6vCUiqKLtd6NfWn0FSvpwrLUyhMu1I1H6U8e0hT/cBN+1zVsJzb5EDJjpRhadrhpPVqEiwzMzORn58visprPUk+QO4BnXpACcuMABVQyhM369R18kHae0DAMgiIXwHcP94kTK+qDy3DUrU3zOA1YVlQUIDCwkIZlGbweTStW1CIWVdeZUcpFaX5TKzbtPq5mT5NZSnwPBrIuNrkQMlPVIalmY1rwpJTlHEt/8k9YOweUCgqUFlWBEWlDEpj922zb09RCZQXAeWFFpseoukzlGGpqXfkfXIPyD0g94DcA3IPyMpSHgNyD8g9IPeA3ANyD2jvgf8P75DMo5Hy44wAAAAASUVORK5CYII="
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes quite a while to train each epoch, so we'll be saving the intermediate model results during the training process. Since fine_tune doesn't do that for us, we'll use fit_one_cycle. Just like cnn_learner, language_model_learner automatically calls freeze when using a pretrained model (which is the default), so this will only train the embeddings (the only part of the model that contains randomly initialized weights—i.e., embeddings for words that are in our IMDb vocab, but aren't in the pretrained model vocab):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>8.227737</td>\n",
       "      <td>7.837894</td>\n",
       "      <td>0.066570</td>\n",
       "      <td>2534.861816</td>\n",
       "      <td>46:32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, 2e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and Loading Models\n",
    "You can easily save the state of your model like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('C:/Users/Naim Cavin/.fastai/data/imdb/models/1epoch.pth')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.save('1epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will create a file in learn.path/models/ named 1epoch.pth. If you want to load your model in another machine after creating your Learner the same way, or resume training later, you can load the content of this file with:\n",
    "\n",
    "learn = learn.load('1epoch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = learn.load('1epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the initial training has completed, we can continue fine-tuning the model after unfreezing:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1' class='' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      10.00% [1/10 51:09<7:40:23]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.773357</td>\n",
       "      <td>4.670860</td>\n",
       "      <td>0.250352</td>\n",
       "      <td>106.789520</td>\n",
       "      <td>51:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='4252' class='' max='33691' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      12.62% [4252/33691 06:11<42:53 4.6865]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-a21773809809>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munfreeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_one_cycle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2e-3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\fastai\\callback\\schedule.py\u001b[0m in \u001b[0;36mfit_one_cycle\u001b[1;34m(self, n_epoch, lr_max, div, div_final, pct_start, wd, moms, cbs, reset_opt)\u001b[0m\n\u001b[0;32m    111\u001b[0m     scheds = {'lr': combined_cos(pct_start, lr_max/div, lr_max, lr_max/div_final),\n\u001b[0;32m    112\u001b[0m               'mom': combined_cos(pct_start, *(self.moms if moms is None else moms))}\n\u001b[1;32m--> 113\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcbs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mParamScheduler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscheds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcbs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset_opt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreset_opt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;31m# Cell\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\fastai\\learner.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, n_epoch, lr, wd, cbs, reset_opt)\u001b[0m\n\u001b[0;32m    219\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_hypers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 221\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_with_events\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_fit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fit'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCancelFitException\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_end_cleanup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_end_cleanup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0myb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\fastai\\learner.py\u001b[0m in \u001b[0;36m_with_events\u001b[1;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_with_events\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevent_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnoop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'before_{event_type}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m  \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mex\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'after_cancel_{event_type}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'after_{event_type}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m  \u001b[0mfinal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\fastai\\learner.py\u001b[0m in \u001b[0;36m_do_fit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_with_events\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'epoch'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCancelEpochException\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcbs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset_opt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\fastai\\learner.py\u001b[0m in \u001b[0;36m_with_events\u001b[1;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_with_events\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevent_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnoop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'before_{event_type}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m  \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mex\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'after_cancel_{event_type}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'after_{event_type}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m  \u001b[0mfinal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\fastai\\learner.py\u001b[0m in \u001b[0;36m_do_epoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_do_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_epoch_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_epoch_validate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\fastai\\learner.py\u001b[0m in \u001b[0;36m_do_epoch_train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_do_epoch_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 198\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_with_events\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall_batches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'train'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCancelTrainException\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_do_epoch_validate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mds_idx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\fastai\\learner.py\u001b[0m in \u001b[0;36m_with_events\u001b[1;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_with_events\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevent_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnoop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'before_{event_type}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m  \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mex\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'after_cancel_{event_type}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'after_{event_type}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m  \u001b[0mfinal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\fastai\\learner.py\u001b[0m in \u001b[0;36mall_batches\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mall_batches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mone_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_do_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\fastai\\learner.py\u001b[0m in \u001b[0;36mone_batch\u001b[1;34m(self, i, b)\u001b[0m\n\u001b[0;32m    192\u001b[0m         \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_with_events\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_one_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'batch'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCancelBatchException\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_do_epoch_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\fastai\\learner.py\u001b[0m in \u001b[0;36m_with_events\u001b[1;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_with_events\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevent_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnoop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'before_{event_type}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m  \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mex\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'after_cancel_{event_type}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'after_{event_type}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m  \u001b[0mfinal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\fastai\\learner.py\u001b[0m in \u001b[0;36m_do_one_batch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0myb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'before_backward'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_grad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_with_events\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'step'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCancelStepException\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    235\u001b[0m         \"\"\"\n\u001b[0;32m    236\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_unary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m             return handle_torch_function(\n\u001b[0m\u001b[0;32m    238\u001b[0m                 \u001b[0mTensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m                 \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\overrides.py\u001b[0m in \u001b[0;36mhandle_torch_function\u001b[1;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1200\u001b[0m         \u001b[1;31m# Use `public_api` instead of `implementation` so __torch_function__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1201\u001b[0m         \u001b[1;31m# implementations can do equality/identity comparisons.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1202\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moverloaded_arg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__torch_function__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpublic_api\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\fastai\\torch_core.py\u001b[0m in \u001b[0;36m__torch_function__\u001b[1;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[0;32m    338\u001b[0m         \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_torch_handled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_opt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtypes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 340\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__torch_function__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    341\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTensorBase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_meta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_copy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36m__torch_function__\u001b[1;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[0;32m    960\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    961\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDisableTorchFunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 962\u001b[1;33m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    963\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 245\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(10, 2e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this is done, we save all of our model except the final layer that converts activations to probabilities of picking each token in our vocabulary. The model not including the final layer is called the encoder. We can save it with save_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save_encoder('finetuned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jargon: Encoder: The model not including the task-specific final layer(s). This term means much the same thing as body when applied to vision CNNs, but \"encoder\" tends to be more used for NLP and generative models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Classifier DataLoaders\n",
    "We're now moving from language model fine-tuning to classifier fine-tuning. To recap, a language model predicts the next word of a document, so it doesn't need any external labels. A classifier, however, predicts some external label—in the case of IMDb, it's the sentiment of a document.\n",
    "\n",
    "This means that the structure of our DataBlock for NLP classification will look very familiar. It's actually nearly the same as we've seen for the many image classification datasets we've worked with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls_clas = DataBlock(\n",
    "    blocks=(TextBlock.from_folder(path, vocab=dls_lm.vocab),CategoryBlock),\n",
    "    get_y = parent_label,\n",
    "    get_items=partial(get_text_files, folders=['train', 'test']),\n",
    "    splitter=GrandparentSplitter(valid_name='test')\n",
    ").dataloaders(path, path=path, bs=128, seq_len=72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls_clas.show_batch(max_n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums_samp = toks200[:10].map(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums_samp.map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5, \n",
    "                                metrics=accuracy).to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1, 2e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sorting and padding are automatically done by the data block API for us when using a TextBlock, with is_lm=False. (We don't have this same issue for language model data, since we concatenate all the documents together first, and then split them into equally sized sections.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5, \n",
    "                                metrics=accuracy).to_fp16()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The final step prior to training the classifier is to load the encoder from our fine-tuned language model. We use load_encoder instead of load because we only have pretrained weights available for the encoder; load by default raises an exception if an incomplete model is loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = learn.load_encoder('finetuned')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
